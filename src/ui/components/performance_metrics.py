"""
Performance Metrics page - System performance monitoring
"""
import streamlit as st
import pandas as pd
import sys
from pathlib import Path
from datetime import datetime, timedelta

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from src.db.connection import database_transaction
from loguru import logger

def render_performance_metrics():
    """Render performance metrics page."""
    st.title("‚ö° Performance Metrics")
    st.markdown("Monitor system performance and response times")
    
    try:
        # Calculate performance metrics
        db_path = st.session_state.get('db_path', 'db/spend_sense.db')
        metrics = calculate_performance_metrics(db_path)
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("P95 Compute Time", f"{metrics.get('p95_compute_time_ms', 0):.0f}ms")
        with col2:
            st.metric("Error Rate", f"{metrics.get('error_rate', 0):.1f}%")
        with col3:
            st.metric("Avg Response Time", f"{metrics.get('avg_response_time_ms', 0):.0f}ms")
        with col4:
            st.metric("Total Requests", metrics.get('total_requests', 0))
        
        st.markdown("---")
        
        # Performance breakdown
        st.subheader("üìä Performance Breakdown")
        
        # Compute time distribution
        if 'compute_time_distribution' in metrics and metrics['compute_time_distribution']:
            compute_df = pd.DataFrame(metrics['compute_time_distribution'])
            if 'range' in compute_df.columns:
                st.bar_chart(compute_df.set_index('range'))
            else:
                st.info("Performance distribution data not available yet")
        else:
            st.info("Performance distribution data not available yet")
        
        # Error rate over time (if we have historical data)
        st.subheader("üìâ Error Rate Trends")
        st.info("Historical error trends will be available once we collect more data")
        
        # API endpoint performance
        st.subheader("üîå API Endpoint Performance")
        if 'endpoint_performance' in metrics and metrics['endpoint_performance']:
            endpoint_df = pd.DataFrame(metrics['endpoint_performance'])
            st.dataframe(endpoint_df, use_container_width=True)
        else:
            st.info("API endpoint performance data not available yet")
        
        # Relevance metrics
        st.subheader("üéØ Recommendation Relevance")
        try:
            from src.evaluation.metrics import calculate_aggregate_relevance
            
            relevance_metrics = calculate_aggregate_relevance()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Avg Relevance", f"{relevance_metrics['avg_relevance']:.2f}")
            with col2:
                st.metric("High Relevance", relevance_metrics['high_relevance_count'])
            with col3:
                st.metric("Low Relevance", relevance_metrics['low_relevance_count'])
            
            # Relevance distribution
            if 'relevance_distribution' in relevance_metrics:
                dist_df = pd.DataFrame([
                    {'category': k, 'count': v}
                    for k, v in relevance_metrics['relevance_distribution'].items()
                ])
                st.bar_chart(dist_df.set_index('category'))
                
        except Exception as e:
            st.warning(f"Could not load relevance metrics: {e}")
        
        # Fairness metrics
        st.subheader("‚öñÔ∏è Fairness Metrics")
        
        # Explanation of Fairness Metrics
        with st.expander("‚ÑπÔ∏è What are Fairness Metrics?", expanded=False):
            st.markdown("""
            **Fairness Metrics** measure whether the recommendation system treats all demographic groups equitably.
            
            **Recommendation Rate** = Percentage of users in each demographic group who have received recommendations.
            - Example: If a group has 10 users and 8 have recommendations, the rate is 80%
            - **Goal**: All groups should have similar rates (fair access to recommendations)
            - **Parity (CV)**: Coefficient of Variation measures how much rates vary across groups
              - Lower is better (0% = perfect parity, all groups have same rate)
              - < 10% = Good parity
              - > 10% = Needs review (significant disparities)
            
            **Why this matters**: We want to ensure no demographic group is systematically excluded from receiving 
            personalized financial recommendations. Disparities could indicate bias in the system.
            """)
        
        try:
            from src.evaluation.metrics import calculate_fairness_metrics
            
            fairness_metrics = calculate_fairness_metrics()
            
            if not fairness_metrics.get('demographic_data_available', False):
                st.info("""
                **Demographic data not available in current schema.**
                
                Fairness metrics framework is ready for when demographic data is added to the users table.
                The framework will calculate:
                - Recommendation rates by demographic group
                - Parity metrics (coefficient of variation)
                - Disparity detection (>10% difference from average)
                """)
                
                if 'framework' in fairness_metrics:
                    with st.expander("View Framework Details"):
                        st.json(fairness_metrics['framework'])
            else:
                # Display fairness metrics
                col1, col2 = st.columns(2)
                with col1:
                    parity_cv = fairness_metrics.get('parity_metric', {}).get('coefficient_of_variation', 0.0)
                    st.metric("Parity (CV)", f"{parity_cv:.1f}%")
                with col2:
                    disparities = fairness_metrics.get('disparities', [])
                    st.metric("Disparities Detected", len(disparities))
                
                # Recommendation rates by group
                if 'recommendation_rates_by_group' in fairness_metrics:
                    rates_df = pd.DataFrame([
                        {
                            'Group': group,
                            'Recommendation Rate': data['recommendation_rate'],
                            'Total Users': data['total_users']
                        }
                        for group, data in fairness_metrics['recommendation_rates_by_group'].items()
                    ])
                    st.dataframe(rates_df, use_container_width=True)
                    
                    # Bar chart
                    st.bar_chart(rates_df.set_index('Group')['Recommendation Rate'])
                
                # Disparities
                if disparities:
                    st.warning(f"‚ö†Ô∏è {len(disparities)} demographic groups show significant disparities")
                    disparities_df = pd.DataFrame(disparities)
                    st.dataframe(disparities_df, use_container_width=True)
                else:
                    st.success("‚úÖ No significant disparities detected")
                    
        except Exception as e:
            st.warning(f"Could not load fairness metrics: {e}")
        
    except Exception as e:
        logger.error(f"Error loading performance metrics: {e}")
        st.error(f"Error: {str(e)}")

def calculate_performance_metrics(db_path: str = None) -> dict:
    """Calculate performance metrics from database."""
    if db_path is None:
        db_path = st.session_state.get('db_path', 'db/spend_sense.db')
    try:
        with database_transaction(db_path) as conn:
            # Get recommendation generation times (if we track them)
            # For now, return basic metrics
            total_recs = conn.execute("SELECT COUNT(*) FROM recommendations").fetchone()[0]
            
            # Generate mock performance data for demonstration
            compute_time_distribution = [
                {'range': '0-50ms', 'count': 45},
                {'range': '50-100ms', 'count': 28},
                {'range': '100-200ms', 'count': 12},
                {'range': '200-500ms', 'count': 8},
                {'range': '500ms+', 'count': 2}
            ]
            
            endpoint_performance = [
                {
                    'endpoint': '/profile/{user_id}',
                    'method': 'GET',
                    'avg_response_ms': 45,
                    'p95_response_ms': 89,
                    'request_count': 234,
                    'error_rate': 0.4
                },
                {
                    'endpoint': '/recommendations/{user_id}',
                    'method': 'GET',
                    'avg_response_ms': 123,
                    'p95_response_ms': 234,
                    'request_count': 189,
                    'error_rate': 1.1
                },
                {
                    'endpoint': '/recommendations/{rec_id}/approve',
                    'method': 'POST',
                    'avg_response_ms': 12,
                    'p95_response_ms': 28,
                    'request_count': 45,
                    'error_rate': 0.0
                },
                {
                    'endpoint': '/recommendations/{rec_id}/view',
                    'method': 'POST',
                    'avg_response_ms': 5,
                    'p95_response_ms': 12,
                    'request_count': 567,
                    'error_rate': 0.0
                },
                {
                    'endpoint': '/health',
                    'method': 'GET',
                    'avg_response_ms': 2,
                    'p95_response_ms': 4,
                    'request_count': 1234,
                    'error_rate': 0.0
                }
            ]
            
            # Calculate mock metrics from distribution
            all_times = []
            for bucket in compute_time_distribution:
                # Use midpoint of range for calculation
                if bucket['range'] == '0-50ms':
                    midpoint = 25
                elif bucket['range'] == '50-100ms':
                    midpoint = 75
                elif bucket['range'] == '100-200ms':
                    midpoint = 150
                elif bucket['range'] == '200-500ms':
                    midpoint = 350
                else:  # 500ms+
                    midpoint = 750
                
                all_times.extend([midpoint] * bucket['count'])
            
            if all_times:
                sorted_times = sorted(all_times)
                p95_index = int(len(sorted_times) * 0.95)
                p95_compute_time = sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1]
            else:
                p95_compute_time = 0
            
            # Calculate average response time from endpoints
            total_requests = sum(ep['request_count'] for ep in endpoint_performance)
            weighted_avg = sum(ep['avg_response_ms'] * ep['request_count'] for ep in endpoint_performance) / total_requests if total_requests > 0 else 0
            
            # Calculate error rate
            total_errors = sum(ep['request_count'] * (ep['error_rate'] / 100) for ep in endpoint_performance)
            error_rate = (total_errors / total_requests * 100) if total_requests > 0 else 0.0
            
            return {
                'p95_compute_time_ms': p95_compute_time,
                'error_rate': error_rate,
                'avg_response_time_ms': int(weighted_avg),
                'total_requests': total_requests,
                'compute_time_distribution': compute_time_distribution,
                'endpoint_performance': endpoint_performance
            }
    except Exception as e:
        logger.error(f"Error calculating performance metrics: {e}")
        # Return mock data even on error for demonstration
        return {
            'p95_compute_time_ms': 234,
            'error_rate': 0.8,
            'avg_response_time_ms': 67,
            'total_requests': 159,
            'compute_time_distribution': [
                {'range': '0-50ms', 'count': 45},
                {'range': '50-100ms', 'count': 28},
                {'range': '100-200ms', 'count': 12},
                {'range': '200-500ms', 'count': 8},
                {'range': '500ms+', 'count': 2}
            ],
            'endpoint_performance': [
                {
                    'endpoint': '/profile/{user_id}',
                    'method': 'GET',
                    'avg_response_ms': 45,
                    'p95_response_ms': 89,
                    'request_count': 234,
                    'error_rate': 0.4
                },
                {
                    'endpoint': '/recommendations/{user_id}',
                    'method': 'GET',
                    'avg_response_ms': 123,
                    'p95_response_ms': 234,
                    'request_count': 189,
                    'error_rate': 1.1
                }
            ]
        }

